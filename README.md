# ğŸ§  LLM Internals Explorer

> **See what's actually happening inside GPT-2 and BERT**

Most LLM tutorials show you APIs. This shows you the math, the attention, the embeddingsâ€”using real models, not simulations.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![React](https://img.shields.io/badge/React-18.3-61DAFB.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688.svg)

---

## Why This Exists

Understanding transformers requires seeing their internals. This tool lets you:

- **Explore Real Models**: Every visualization uses actual GPT-2/BERT inferenceâ€”no fake data
- **Discover Head Personalities**: See that GPT-2's 144 attention heads learned different linguistic roles
- **Understand Embeddings**: Perform vector arithmetic (King - Man + Woman = Queen) in 3D space

---

## ğŸ¯ Headline Feature: Attention Head Personality Profiler

**The Discovery**: GPT-2's 144 attention heads aren't uniformâ€”they've specialized.

- **Syntax Trackers** (Cluster 0): Monitor grammatical structure and agreement
- **Semantic Linkers** (Cluster 1): Connect related concepts across sentences  
- **Positional Encoders** (Cluster 2): Track word order and sequence
- **Rare Pattern Detectors** (Cluster 3): Identify unusual linguistic constructions
- **Context Aggregators** (Cluster 4): Build holistic sentence representations

**How it works**: We run 50 diverse sentences through GPT-2, extract behavioral features from each head's attention patterns, then cluster them using t-SNE. Click any head in the 3D visualization to see example sentences showing what it does.

---

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt
cd frontend && npm install

# Set up environment (optional: for AI-powered security)
echo "OPENROUTER_API_KEY=your_key_here" > .env

# Run backend
cd api
python main.py  # API at http://localhost:8000

# Run frontend (new terminal)
cd frontend
npm run dev  # UI at http://localhost:5173
```

---

## Features

### ğŸ¯ Attention Head Profiler (NEW)
Discover that GPT-2's 144 attention heads have learned different roles. See them clustered by behavior in interactive 3D.

### ğŸ” Real Attention Weights  
View actual GPT-2 attention across all 12 layers Ã— 12 heads. See which words attend to which.

### ğŸ“Š Semantic Vector Space
Explore word embeddings in 3D. Perform vector arithmetic and see the geometry of meaning.

### ğŸ”¤ Tokenizer Lab
Watch BPE tokenization happen in real-time. Understand why "hello" is one token but "antidisestablishmentarianism" is many.

### ğŸ—ï¸ Architecture Overview
Interactive transformer diagram showing embeddings â†’ attention â†’ FFN â†’ output.

### ğŸ“‰ Context Window Analysis
Visualize the "Lost in the Middle" phenomenonâ€”how recall degrades at different context positions.

### ğŸ”’ AI Security Analyzer
LLM-powered prompt injection detection using Agno agents (fallback to heuristics if no API key).

### ğŸ’° Cost Router
Compare LLM API costs across providers (GPT-4, Claude, Llama). Understand token pricing.

---

## Technical Stack

**Models**: GPT-2 (124M params), all-MiniLM-L6-v2 (sentence transformers)  
**Backend**: FastAPI, PyTorch, Transformers, scikit-learn  
**Frontend**: React, TypeScript, Vite, TailwindCSS, React Three Fiber  
**Visualization**: Plotly, t-SNE clustering  

---

## Educational Depth

Each module includes:
- âœ… Mathematical formulas (attention equation, softmax, PCA)
- âœ… Real model outputs (not simulations)
- âœ… Interactive experiments
- âœ… Source code you can read

---

## Project Structure

```
llm_workflow/
â”œâ”€â”€ llm_logic.py              # Core LLM mechanics
â”œâ”€â”€ attention_profiler.py     # Head clustering & analysis
â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ api/                      # FastAPI backend
â”‚   â”œâ”€â”€ main.py              # API entry point
â”‚   â””â”€â”€ routers/             # Endpoints
â”‚       â”œâ”€â”€ attention.py     # Attention + head profiler
â”‚       â”œâ”€â”€ embeddings.py    # Semantic space
â”‚       â”œâ”€â”€ tokenizer.py     # BPE tokenization
â”‚       â”œâ”€â”€ security.py      # AI security analysis
â”‚       â”œâ”€â”€ cost.py          # Cost comparison
â”‚       â””â”€â”€ context.py       # Context window
â”‚
â””â”€â”€ frontend/                 # React app
    â””â”€â”€ src/components/modules/
        â”œâ”€â”€ AttentionModule.tsx
        â”œâ”€â”€ EmbeddingsModule.tsx
        â”œâ”€â”€ TokenizerModule.tsx
        â””â”€â”€ ...
```

---

## API Documentation

Once running, visit **http://localhost:8000/docs** for interactive API docs.

### Key Endpoints

```http
POST /api/attention/analyze
GET  /api/attention/head-profiles        # 3D head clustering
GET  /api/attention/head-examples/{layer}/{head}
POST /api/embeddings/generate
POST /api/security/analyze
POST /api/cost/analyze
```

---

## What Makes This Different

âŒ **Not a demo**: Real GPT-2 inference, not simulated attention  
âŒ **Not surface-level**: Shows the math and the mechanics  
âŒ **Not just visualization**: Reveals non-obvious insights (head specialization)  

âœ… **Educational tool**: Built for engineers who want to understand, not just use  
âœ… **Real models**: Every number comes from actual transformer forward passes  
âœ… **Novel insights**: Attention head clustering is genuinely rare  

---

## Contributing

Focus areas:
- More attention head analysis patterns
- Additional transformer architectures (BERT, T5, Llama)
- Performance optimizations for large sequences
- More canonical sentences for head profiling

---

## Requirements

- Python 3.8+
- Node.js 18+
- 4GB RAM (for GPT-2 inference)
- Optional: OpenRouter API key (for AI security analysis)

---

## License

MIT

---

## Acknowledgments

- **HuggingFace Transformers** for pre-trained models
- **OpenAI tiktoken** for tokenization
- **Sentence Transformers** for embeddings
- **Plotly** for visualizations
- **shadcn/ui** for React components

---

**Built to teach, not to impress.**

[Report Bug](https://github.com/yourusername/llm_workflow/issues) Â· [Request Feature](https://github.com/yourusername/llm_workflow/issues)
